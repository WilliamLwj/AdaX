# AdaX: Adaptive Gradient Descent with Exponential Long Term Momery

A new adaptive optimizer that can run faster than Stochastic Gradient Descent with momentum (SGDM) and get similar performance in various computer vision and natural language processing tasks.

Please use AdaXW by


```python3
from AdaX import AdaXW
```
